<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>...</title>
    <style>
      .copiedMsg {
        display: none;
        color: green;
        margin-left: 10px;
      }
    </style>
  </head>
  <body>
    <ul>
      <li>
        <span>Lab 1</span>
        <button class="copyBtn" data-lab="1">Copy</button>
        <span class="copiedMsg" id="copiedLab1">Copied!</span>
      </li>
      <br />

      <script>
        for (let i = 2; i <= 10; i++) {
          document.write(`
                    <li>
                        <span>Lab ${i}</span>
                        <button class="copyBtn" data-lab="${i}">Copy</button>
                        <span class="copiedMsg" id="copiedLab${i}">Copied!</span>
                        
                    </li>
                    <br>
                `)
        }
      </script>
    </ul>

    <script>
      // Define lab content using an object
      const labContents = {
        1: `import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing

data=fetch_california_housing(as_frame=True)
housing_df=data.frame

numerical_feature=housing_df.select_dtypes(include=np.number).columns

plt.figure(figsize=(15,10))
for i,feature in enumerate(numerical_feature):
  plt.subplot(3,3,i+1)
  sns.histplot(housing_df[feature],kde=True,bins=30,color='blue')
  plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

plt.figure(figsize=(15,10))
for i,feature in enumerate(numerical_feature):
  plt.subplot(3,3,i+1)
  sns.boxplot(x=housing_df[feature],color='orange')
  plt.title(f'Box plot of {feature}')
plt.tight_layout()
plt.show()

print('outliers summary')

outliers_summary={}
for feature in numerical_feature:
  Q1=housing_df[feature].quantile(0.25)
  Q3=housing_df[feature].quantile(0.75)
  IQR=Q3-Q1
  lower_bound=Q1-1.5*IQR
  upper_bound=Q3+1.5*IQR
  outliers=housing_df[(housing_df[feature]<lower_bound)|(housing_df[feature]>upper_bound)]

  outliers_summary[feature]=len(outliers)
  print(f"{feature}:{len(outliers)}outliers")

  print("\n dataset summary")
  print(housing_df.describe())`,
        2: `import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing

california_data=fetch_california_housing(as_frame=True)
data=california_data.frame

correlation_matrix=data.corr()

plt.figure(figsize=(10,8))
sns.heatmap(correlation_matrix,annot=True,cmap='coolwarm',fmt='.2f',linewidths=0.5)
plt.title("correlation of california housing")
plt.show()


sns.pairplot(data,diag_kind='kde',plot_kws={'alpha':0.5})
plt.suptitle("pairplot of california housing ",y=1.02)
plt.show()`,
        3: `import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

iris = load_iris()
data = iris.data
labels = iris.target
label_names = iris.target_names

iris_df = pd.DataFrame(data, columns=iris.feature_names)


pca = PCA(n_components=2)
data_reduced = pca.fit_transform(data)

reduced_df = pd.DataFrame(data_reduced, columns=['Principal Component 1', 'Principal Component 2'])
reduced_df['Label'] = labels

plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']
for i, label in enumerate(np.unique(labels)):
  plt.scatter(
      reduced_df[reduced_df['Label'] == label]['Principal Component 1'],
      reduced_df[reduced_df['Label'] == label]['Principal Component 2'],
      label=label_names[label],
      color=colors[i]
  )
plt.title('PCA on Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid()
plt.show()
`,
        4: `def find_s(examples):
  hypothesis=['ψ','ψ','ψ','ψ','ψ','ψ',]

  for example in examples:
    if example[-1]=='yes':
      for i in range(len(hypothesis)):
        if hypothesis[i]=='ψ':
          hypothesis[i]=example[i]
        elif hypothesis[i]!=example[i]:
          hypothesis[i]='?'
  return hypothesis

data=[
    ['sunny','warm','normal','strong','warm','same','yes'],
    ['sunny','warm','high','strong','warm','same','yes'],
    ['rainy','cold','high','strong','warm','change','no'],
    ['sunny','warm','high','strong','cool','change','yes']
]
hypothesis=find_s(data)
print("the new hypothesis is:",hypothesis)
`,
        5: `import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

data=np.random.rand(100)
train,test=data[:50],data[50:]
labels=[("class1" if x<=0.5 else "class2") for x in train]

def knn(x,X,y,k):
  dists=[(abs(x-xi),yi)for xi,yi in zip(X,y)]
  top_k=sorted(dists)[:k]
  return Counter([label for _, label in top_k]).most_common(1)[0][0]


k_vals=[1,2,3,4,5,20,30]
result={}
print("K-NN classification")
for k in k_vals:
  print("results for k={k}")
  res=[knn(x,train,labels,k) for x in test ]
  result[k]=res
  for i, label in enumerate(res,51):
    print(f"point x{i} (value:{(i-51):.4f}), is classified as {label}")


for k in k_vals:
  res=result[k]
  c1=[test[i] for i in range(50) if res[i]=="class1"]
  c2=[test[i] for i in range (50)if res[i]=="class2"]
  plt.figure(figsize=(10,5))
  plt.scatter(train, [0]*50, c=["b" if l =="class1" else "r" for l in labels], label="Train",marker="o")
  plt.scatter(c1,[1]*len(c1),c="b",label="class1(Test)",marker="x")
  plt.scatter(c2,[1]*len(c2),c="r",label="class2(Test)",marker="x")
  plt.title(f"K-NN classification for k={k}")
  plt.xlabel("Data points")
  plt.yticks([0,1],["Train","Test"])
  plt.legend()
  plt.grid(True)
  plt.show()`,
        6: `import numpy as np
import matplotlib.pyplot as plt

def gaussian(x,xi,tau):
  return np.exp(-np.sum((x-xi)**2)/(2*tau**2))

def lwr(x,X,y,tau):
  W=np.diag([gaussian(x,xi,tau) for xi in X])
  theta=np.linalg.inv(X.T @ W @ X) @ X.T @ W @ y
  return x @ theta

np.random.seed(42)
X=np.linspace(0,2*np.pi,100)
y=np.sin(X)+0.1 * np.random.randn(100)
Xb=np.c_[np.ones(X.shape),X]

xt=np.linspace(0,2*np.pi,200)
xtb=np.c_[np.ones(xt.shape),xt]
tau=0.5
yp=np.array([lwr(xi,Xb,y,tau) for xi in xtb])

plt.figure(figsize=(10,6))
plt.scatter(X,y,c='r',label='Data',alpha=0.7)
plt.plot(xt,yp,c='b',label=f'LWR (tau={tau})')
plt.title('locally weighted regression')
plt.xlabel('X')
plt.ylabel('y')
plt.grid(alpha=0.3)
plt.legend()
plt.show()`,
        7: `import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, r2_score
def linear_regression_california():
  housing = fetch_california_housing(as_frame=True)
  X = housing.data[["AveRooms"]]
  y = housing.target
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  model = LinearRegression()
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  plt.scatter(X_test, y_test, color="blue", label="Actual")
  plt.plot(X_test, y_pred, color="red", label="Predicted")
  plt.xlabel("Average number of rooms (AveRooms)")
  plt.ylabel("Median value of homes ($100,000)")
  plt.title("Linear Regression - California Housing Dataset")
  plt.legend()
plt.show()
print("Linear Regression - California Housing Dataset")
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R^2 Score:", r2_score(y_test, y_pred))
def polynomial_regression_auto_mpg():
  url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
  column_names = ["mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin"]
  data = pd.read_csv(url, sep='\s+', names=column_names, na_values="?")
  data = data.dropna()
  X = data["displacement"].values.reshape(-1, 1)
  y = data["mpg"].values
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  poly_model = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), LinearRegression())
  poly_model.fit(X_train, y_train)

  y_pred = poly_model.predict(X_test)
  plt.scatter(X_test, y_test, color="blue", label="Actual")
  plt.scatter(X_test, y_pred, color="red", label="Predicted")
  plt.xlabel("Displacement")
  plt.ylabel("Miles per gallon (mpg)")
  plt.title("Polynomial Regression - Auto MPG Dataset")
  plt.legend()
  plt.show()
print("Polynomial Regression - Auto MPG Dataset")
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R^2 Score:", r2_score(y_test, y_pred))
if __name__ == "__main__":
  print("Demonstrating Linear Regression and Polynomial Regression\n")
  linear_regression_california()
  polynomial_regression_auto_mpg()
`,
        8: `import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree

data=load_breast_cancer()
X=data.data
y= data.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
clf=DecisionTreeClassifier(random_state=42)
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)

accuracy=accuracy_score(y_test,y_pred)
print(f"accuracy:{accuracy*100:.2f}%")
new_sample=np.array([X_test[0]])
prediction=clf.predict(new_sample)

prediction_class="Benign" if prediction==1 else "Malignant"
print(f"prediction class of dataset:{prediction_class}")

plt.figure(figsize=(12,8))
tree.plot_tree(clf,filled=True,feature_names=data.feature_names,class_names=data.target_names)
plt.title("Decision tree breast cancer dataset")
plt.show()`,
        9: `import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

data=fetch_olivetti_faces()
X=data.data
y=data.target
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

gnb=GaussianNB()
gnb.fit(X_train,y_train)
y_pred=gnb.predict(X_test)

accuracy=accuracy_score(y_test,y_pred)
print(f"accuracy is {accuracy*100:.2f}%")

print("classification Report:")
print(classification_report(y_test,y_pred,zero_division=1))

print("confusion Matrix:")
print(confusion_matrix(y_test,y_pred))

cross_val_accuracy=cross_val_score(gnb,X,y,cv=5,scoring='accuracy')
print("cross validation score:{cross_accuracy_score.mean() * 100:.2f}%")

fig,axes=plt.subplots(3,5,figsize=(12,8))
for ax,image,label,prediction in zip(axes.ravel(),X_test,y_test,y_pred):
  ax.imshow(image.reshape(64,64),cmap=plt.cm.grey)
  ax.set_title(f"True:{label},pred:{prediction}")
  ax.axis('off')
plt.show()






`,
        10: `import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, classification_report
data = load_breast_cancer()
X = data.data
y = data.target
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=2, random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)
print("Confusion Matrix:")
print(confusion_matrix(y, y_kmeans))
print("\nClassification Report:")
print(classification_report(y, y_kmeans))
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df['Cluster'] = y_kmeans
df['True Label'] = y
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black', alpha=0.7)
plt.title('K-Means Clustering of Breast Cancer Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title="Cluster")
plt.show()
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='PC1', y='PC2', hue='True Label', palette='coolwarm', s=100, edgecolor='black',
alpha=0.7)
plt.title('True Labels of Breast Cancer Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title="True Label")
plt.show()
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black', alpha=0.7)
centers = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centroids')
plt.title('K-Means Clustering with Centroids')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title="Cluster")
plt.show()`,
      }

      document.addEventListener("DOMContentLoaded", function () {
        document.querySelectorAll(".copyBtn").forEach((button) => {
          button.addEventListener("click", function () {
            let labNumber = this.getAttribute("data-lab")
            let labContent = labContents[labNumber]

            // Copy content to clipboard
            navigator.clipboard.writeText(labContent).then(() => {
              let copiedSpan = document.getElementById(`copiedLab${labNumber}`)
              copiedSpan.style.display = "inline" // Show the span

              setTimeout(() => {
                copiedSpan.style.display = "none" // Hide the span after 2 seconds
              }, 2000)
            })
          })
        })
      })
    </script>
  </body>
</html>
